{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims at training a custom fasttext word embedding model. It uses another git located [here](https://github.com/alexandredupuy-zini/dynamic-topic-modeling), make sure that `kedro==0.15.4` is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('dynamic-topic-modeling/')\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '45'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m kedro run --pipeline get_most_similar_fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# Imports and additionnal processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset_tweet.csv\", sep=\"|\", parse_dates=['timestamp'], dtype={\"id\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_text(text, stopwords):\n",
    "    text = text.lower() # convert text to lower-case\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub('@[^\\s]+', '', text) # remove usernames\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text) # remove the # in #hashtag\n",
    "    #text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text) # remove non-ASCII characters\n",
    "    tkz = nltk.RegexpTokenizer(\"\\\\b[\\\\w-]+\\\\b\")\n",
    "    text = tkz.tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words and not word.isnumeric() and len(word) > 2]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stopwords = [\"coronavirus\", \"19\", \"ici\", \"via\", \"selon\", \"plus\", \"Ã§a\", \"pers\", \"blumenthal\",\n",
    "                 \"not\", \"against\", \"fauci\", \"the\", \"maxi\", \"nan\"]\n",
    "\n",
    "stop_words = stopwords.words(\"french\") + add_stopwords\n",
    "stop_words.remove(\"pas\")\n",
    "stop_words.remove(\"ne\")\n",
    "\n",
    "df[\"text\"] = df[\"text\"].swifter.apply(lambda x: tidy_text(str(x), stopwords))\n",
    "df[\"date\"] = df[\"timestamp\"].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word selection by iteration\n",
    "\n",
    "This section aims at enriching the vocabulary with covid-related terms, as it was learned by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = fasttext.load_model(\"data/06_models/fasttext_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim_from_model(word, model, top_n=20) :\n",
    "    cs=defaultdict()\n",
    "    wv=model[word]\n",
    "    if type(model)==fasttext.FastText._FastText :\n",
    "        all_words=model.words\n",
    "    else :\n",
    "        all_words=list(model.wv.vocab.keys())\n",
    "    for words in [i for i in all_words if i!=word] :\n",
    "        curr_wv=model[words]\n",
    "        cs[words]=cosine_similarity(wv.reshape(1,-1),curr_wv.reshape(1,-1)).flatten()[0]\n",
    "    sorted_cs = dict(sorted(cs.items(), key=lambda kv: kv[1],reverse=True)[:top_n])\n",
    "    return sorted_cs\n",
    "\n",
    "def get_most_similar(model, words):\n",
    "    total = {}\n",
    "    for word in tqdm(words):\n",
    "        for key, value in get_cos_sim_from_model(word,model).items():\n",
    "            if key in total.keys():\n",
    "                if value > total[key]:\n",
    "                    total[key] = value\n",
    "            else:\n",
    "                add = 0\n",
    "                for w in words:\n",
    "                    add += len(re.findall(w, key))\n",
    "                if add == 0:\n",
    "                    total[key] = value\n",
    "    return({k: v for k, v in sorted(total.items(), key=lambda item: item[1], reverse=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This list of words was iteratively completed starting with the word \"vaccin\", using the following function\n",
    "words = [\"vaccin\", \"moderna\", \"astrazeneca\", \"pfizer\", \"gsk\", \"dose\", 'sanofi', \"oxford\", \"pharmaceutique\",\n",
    "         \"gilead\", \"medicament\", \"novartis\", \"medoc\", \"remdesivir\", \"bigpharma\", \"traitement\", \"chloroquine\",\n",
    "         \"hydroxychloroquine\", \"plaquenil\", \"azithromycin\", \"raoult_didier\", \"azythromycin\", \"raoult\", \"lancet\",\n",
    "         \"didier\", \"antiviral\", \"antibiotique\", \"etude\", \"labos\", \"dexamethasone\", \"lobbies\", \"antiviraux\",\n",
    "         \"lobby\", \"remede\", \"gates\", \"corrompu\", \"cobaye\", \"tisane\", \"artemisia\", \"charlatan\", \"puce\",\n",
    "         \"rfid\", \"surgisphere\", 'soros', 'virolog', \"prophylaxie\", 'potion', 'miracle', 'tocilizumab', 'antidote',\n",
    "         \"automedication\", 'interet', '5g', 'inject', 'antenne', 'complot', 'competen', 'toxi', 'conspiration',\n",
    "         'steroide', 'theorie', \"mondialiste\", 'traitre', 'essais', 'escroc', 'soigner', 'blackrock', 'conflit']\n",
    "\n",
    "get_most_similar(model, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many texts contain the selected words, and keep their ids\n",
    "\n",
    "idx_words = []\n",
    "idx_vaccin = []\n",
    "\n",
    "for key in words:\n",
    "    found = 0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if len(re.findall(key, str(df['text'].iloc[i])))>0:\n",
    "            found += 1\n",
    "            if key == \"vaccin\":\n",
    "                idx_vaccin.append(i)\n",
    "            idx_words.append(i)\n",
    "\n",
    "    print(\"Word {} is in {} texts\".format(key, found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two words are added only for the purpose of visualisation.\n",
    "words_tmp = words + [\"vacciner\", \"vaccination\"]\n",
    "inputs = [model.get_word_vector(word) for word in words_tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=2020)\n",
    "outputs = pca.fit_transform(inputs)\n",
    "\n",
    "df_ft = pd.DataFrame(outputs, index=words_tmp)\n",
    "\n",
    "ax = df_ft.plot(x=0, y=1, style='o', kind=\"scatter\", figsize=(20,10))\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    if words_tmp[i] == \"vaccin\":\n",
    "        ax.text(outputs[i][0], outputs[i][1], words_tmp[i], c='r')\n",
    "    elif words_tmp[i] in [\"vacciner\", \"vaccination\"]:\n",
    "        ax.text(outputs[i][0], outputs[i][1], words_tmp[i], c='b')\n",
    "    else:\n",
    "        ax.text(outputs[i][0], outputs[i][1], words_tmp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(idx_words, open(\"idx_words.pkl\", 'wb'))\n",
    "pickle.dump(idx_vaccin, open(\"idx_vaccin.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['Unnamed: 0'], inplace=True)\n",
    "header = list(df.columns.values)\n",
    "\n",
    "df.iloc[idx_words].to_csv('../sentiment_dataset/dataset_small.csv', sep=\"|\", header=header)\n",
    "df.iloc[idx_vaccin].to_csv('../sentiment_dataset/dataset_vaccin.csv', sep=\"|\", header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
