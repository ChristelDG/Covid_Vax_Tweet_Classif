# Parameters for the preprocessing pipeline.
extreme_no_below: 300 #if > : for a word w, delete this word from vocabulary if w in less than extreme_no_below documents. if in [0,1], for a word w, delete this word from vocabulary if w in less than extreme_no_below% documents
extreme_no_above: 0.7 #in [0,1], for a word w, delete this word from vocabulary if w in more than extreme_no_below% documents
enable_bigram: False # Boolean, decide if you want bigrams or not in the dictionary
min_bigram_count: 200 #Int, threshold for bigrams :  Bigram will be added to the dictionary if in more than min_bigram_count documents
basic_word_analysis: False #Boolean, set to True if you want to print some basic word anaylis (basically the number of words removed from each preprocces steps.)
lemmatize : False
temporality : week #if set to week, set a timeslice every week. If set to month, set a timeslice every month. If set to year, set a timeslice every year. WARNING : when set to year, expect yearly observations.
language : fr #language of text in data
split_by_paragraph: False #set to True if documents need to be split by paragraphs
additionnal_stop_words: ["amp", "</s>", "via"] #Stop words you might want to add


# Parameters for train test val split
test_size: 0.10
val_size: 0.05


# Parameters related to fasttext embeddings
dim : 300
window : 6
model : skipgram
iterations : 10
min_count: 8
path_to_texts_for_embedding: data/05_model_input/texts_used_for_embeddings.txt ## DO NOT CHANGE this parameter.
param_grid : {'ws' : [2,4,5,6,7,8], 'epoch' : [3,5,10,15], 'minCount' : [2,3,4,6,8]}
word_to_check: 'vaccin'
ft_cpu_thread: 40
couple_list: [["covid","coronavirus"], ["epidemie","pandemie"], ["president","republique"], ["crise","sanitaire"], ["traitement","vaccin"], ["soignants","infirmiers"], ["deces","morts"], ["mai","avril"], ["vague","seconde"], ["chloroquine","raoult"], ["cas","confirmes"], ["gels","masques"], ["gestes","barrieres"], ["positifs","detectes"], ["frontieres","touristes"], ["immunite","collective"], ["quarantaine","isolement"], ["penurie","manque"], ["conseil","scientifique"], ["bill","gates"], ["chomage","partiel"], ["urgence","sanitaire"], ["attestation","deplacement"], ["departement","region"]]

#Parameter related to get the most similar terms from fasttext
words: ["vaccin", "chloroquine", "remede", "moderna", "cobayes", "traitement", "raoult", "lancet", "medicament", 'lancetgate', "5g", 'billgates', 'puce', 'soros', 'huawei', 'complot', 'gilead', 'lobbies', "id2020", "identification", 'bigpharma', 'vaccination', "vacciner", "vaccinale"]

# Parameters related to DETM model
pretrained_embeddings : True
num_topics : 5
emb_size : 300
t_hidden_size : 800  # dimension of hidden space of q(theta)
eta_hidden_size : 400 #number of hidden units for rnn
rho_size : 300 # Dimension of topic embeddings. should be similar to emb_size
emb_size : 300 # Dimension of pretrained embeddings
enc_drop : 0.1 # dropout rate of encoder
eta_nlayers: 4 # Number of layers for LSTM when infering eta
eta_dropout : 0.0 #dropout rate of RNN when infering eta
train_embeddings : True #wheter to fix rho or train it
theta_act : relu #tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu
delta : 0.005 #Prior variance for eta inference
gamma2 : 0.005 #Prior variance for alpha inference
GPU : True #Whether to train on GPU or CPU.


# Parameters related to training DETM
n_epochs : 150 #Number of epochs
batch_size : 128 #Number of documents in train batch
optimizer : adam #choice of optimizer
learning_rate : 0.001 #learning rate
eval_metric : perplexity #Choice of eval mertric for early stopping & annealing. Must be one of [perplexity, TQ, TD, TC]
early_stopping : True #whether to stop training if validation score stops decreasing
early_stopping_rounds : 10 #Max number of epochs allowed with a validation score higher than best validation score before stopping training
log_interval : 500 #print batch loss logs every log interval
wdecay : 1.2e-6 #L2 regularization on optimizer
eval_batch_size : 128 #Number of documents in batch when evaluating test & val perplexity
anneal_lr : True #wheter to anneal learning rate if loss is increasing
lr_factor : 4.0 #divide learning rate by this when anneal_lr is set to True
nonmono : 5 #divide learning rate by lr_factor if anneal_lr is true when val perplexity was not better than the best perplexity during the last "nonmono" epochs
clip_grad : 2.0 #clip gradient
seed : 2020 #setting seed for reproductible results


# Parameters for evaluation/reporting
num_diversity : 25 # Number of words to take account of when computing diversity. Higher number means better diversity
num_coherence : 10


# Words query for plotting word evolution. The convention is that topic 1 represents topic 0
query : {
        '1' : ['etude', 'hydroxychloroquine', 'raoult', 'traitement', 'vaccin'],
        '2' : ['vaccin', 'gates', 'oms', 'premier', 'russie', 'masque', 'essais', 'desobeissance', 'traitement', 'temps'],
        '3' : ['raoult', 'hydroxychloroquine', 'chloroquine','etude', 'lancet', 'patients', 'vaccination', 'peur'],
        '4' : ['etude', 'chloroquine', 'gouvernement', 'traitement', 'cas'],
        '5' : ['etude', 'hydroxychloroquine', 'traitement', 'raoult', 'gouvernement'],
        }
